<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>GPU Computing with Rust using CUDA</title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="Writing extremely fast GPU Computing code with rust using rustc_codegen_nvvm and CUDA">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="features.html"><strong aria-hidden="true">2.</strong> Supported Features</a></li><li class="chapter-item expanded "><a href="faq.html"><strong aria-hidden="true">3.</strong> Frequently Asked Questions</a></li><li class="chapter-item expanded "><a href="guide/index.html"><strong aria-hidden="true">4.</strong> Guide</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="guide/getting_started.html"><strong aria-hidden="true">4.1.</strong> Getting Started</a></li><li class="chapter-item expanded "><a href="guide/tips.html"><strong aria-hidden="true">4.2.</strong> Tips</a></li><li class="chapter-item expanded "><a href="guide/kernel_abi.html"><strong aria-hidden="true">4.3.</strong> Kernel ABI</a></li><li class="chapter-item expanded "><a href="guide/safety.html"><strong aria-hidden="true">4.4.</strong> Safety</a></li></ol></li><li class="chapter-item expanded "><a href="cuda/index.html"><strong aria-hidden="true">5.</strong> The CUDA Toolkit</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="cuda/gpu_computing.html"><strong aria-hidden="true">5.1.</strong> GPU Computing</a></li><li class="chapter-item expanded "><a href="cuda/pipeline.html"><strong aria-hidden="true">5.2.</strong> The CUDA Pipeline</a></li></ol></li><li class="chapter-item expanded "><a href="nvvm/index.html"><strong aria-hidden="true">6.</strong> rustc_codegen_nvvm</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="nvvm/technical/index.html"><strong aria-hidden="true">6.1.</strong> Technical</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="nvvm/technical/backends.html"><strong aria-hidden="true">6.1.1.</strong> Custom Rustc Backends</a></li><li class="chapter-item expanded "><a href="nvvm/technical/nvvm.html"><strong aria-hidden="true">6.1.2.</strong> rustc_codegen_nvvm</a></li><li class="chapter-item expanded "><a href="nvvm/technical/types.html"><strong aria-hidden="true">6.1.3.</strong> Types</a></li></ol></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">GPU Computing with Rust using CUDA</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div style="break-before: page; page-break-before: always;"></div><h1 id="supported-features"><a class="header" href="#supported-features">Supported Features</a></h1>
<p>This page is used for tracking Cargo/Rust and CUDA features that are currently supported 
or planned to be supported in the future. As well as tracking some information about how they could 
be supported.</p>
<p>Note that <code>Not supported</code> does <strong>not</strong> mean it won't ever be supported, it just means we haven't gotten
around to adding it yet.</p>
<div class="table-wrapper"><table><thead><tr><th>Indicator</th><th>Meaning</th></tr></thead><tbody>
<tr><td>➖</td><td>Not Applicable</td></tr>
<tr><td>❌</td><td>Not Supported</td></tr>
<tr><td>✔️</td><td>Fully Supported</td></tr>
<tr><td>🟨</td><td>Partially Supported</td></tr>
</tbody></table>
</div>
<h1 id="rust-features"><a class="header" href="#rust-features">Rust Features</a></h1>
<div class="table-wrapper"><table><thead><tr><th>Feature Name</th><th>Support Level</th><th>Notes</th></tr></thead><tbody>
<tr><td>Opt-Levels</td><td>✔️</td><td>behaves mostly the same (because llvm is still used for optimizations). Except that libnvvm opts are run on anything except no-opts because nvvm only has -O0 and -O3</td></tr>
<tr><td>codegen-units</td><td>✔️</td><td></td></tr>
<tr><td>LTO</td><td>➖</td><td>we load bitcode modules lazily using dependency graphs, which then forms a single module optimized by libnvvm, so all the benefits of LTO are on without pre-libnvvm LTO being needed.</td></tr>
<tr><td>Closures</td><td>✔️</td><td></td></tr>
<tr><td>Enums</td><td>✔️</td><td></td></tr>
<tr><td>Loops</td><td>✔️</td><td></td></tr>
<tr><td>If</td><td>✔️</td><td></td></tr>
<tr><td>Match</td><td>✔️</td><td></td></tr>
<tr><td>Proc Macros</td><td>✔️</td><td></td></tr>
<tr><td>Try (<code>?</code>)</td><td>✔️</td><td></td></tr>
<tr><td>128 bit integers</td><td>🟨</td><td>Basic ops should work (and are emulated), advanced intrinsics like <code>ctpop</code>, <code>rotate</code>, etc are unsupported.</td></tr>
<tr><td>Unions</td><td>✔️</td><td></td></tr>
<tr><td>Iterators</td><td>✔️</td><td></td></tr>
<tr><td>Dynamic Dispatch</td><td>✔️</td><td></td></tr>
<tr><td>Pointer Casts</td><td>✔️</td><td></td></tr>
<tr><td>Unsized Slices</td><td>✔️</td><td></td></tr>
<tr><td>Alloc</td><td>✔️</td><td></td></tr>
<tr><td>Printing</td><td>✔️</td><td></td></tr>
<tr><td>Panicking</td><td>✔️</td><td>Currently just traps (aborts) because of weird printing failures in the panic handler</td></tr>
<tr><td>Float Ops</td><td>✔️</td><td>Maps to libdevice intrinsics, calls to libm are not intercepted though, which we may want to do in the future</td></tr>
<tr><td>Atomics</td><td>❌</td><td></td></tr>
</tbody></table>
</div>
<h1 id="cuda-libraries"><a class="header" href="#cuda-libraries">CUDA Libraries</a></h1>
<div class="table-wrapper"><table><thead><tr><th>Library Name</th><th>Support Level</th><th>Notes</th></tr></thead><tbody>
<tr><td>CUDA Runtime API</td><td>➖</td><td>The CUDA Runtime API is for CUDA C++, we use the driver API</td></tr>
<tr><td>CUDA Driver API</td><td>🟨</td><td>Most functions are implemented, but there is still a lot left to wrap because it is gigantic</td></tr>
<tr><td>cuBLAS</td><td>❌</td><td>In-progress</td></tr>
<tr><td>cuFFT</td><td>❌</td><td></td></tr>
<tr><td>cuSOLVER</td><td>❌</td><td></td></tr>
<tr><td>cuRAND</td><td>➖</td><td>cuRAND only works with the runtime API, we have our own general purpose GPU rand library called <code>gpu_rand</code></td></tr>
<tr><td>cuDNN</td><td>❌</td><td>In-progress</td></tr>
<tr><td>cuSPARSE</td><td>❌</td><td></td></tr>
<tr><td>AmgX</td><td>❌</td><td></td></tr>
<tr><td>cuTENSOR</td><td>❌</td><td></td></tr>
<tr><td>OptiX</td><td>🟨</td><td>CPU OptiX is mostly complete, GPU OptiX is still heavily in-progress because it needs support from the codegen</td></tr>
</tbody></table>
</div>
<h1 id="gpu-side-features"><a class="header" href="#gpu-side-features">GPU-side Features</a></h1>
<p>Note: Most of these categories are used <strong>very</strong> rarely in CUDA code, therefore
do not be alarmed that it seems like many things are not supported. We just focus
on things used by the wide majority of users.</p>
<div class="table-wrapper"><table><thead><tr><th>Feature Name</th><th>Support Level</th><th>Notes</th></tr></thead><tbody>
<tr><td>Function Execution Space Specifiers</td><td>➖</td><td></td></tr>
<tr><td>Variable Memory Space Specifiers</td><td>✔️</td><td>Handled Implicitly but can be explicitly stated for statics with <code>#[address_space(...)]</code></td></tr>
<tr><td>Built-in Vector Types</td><td>➖</td><td>Use linear algebra libraries like vek or glam</td></tr>
<tr><td>Built-in Variables</td><td>✔️</td><td></td></tr>
<tr><td>Memory Fence Instructions</td><td>✔️</td><td></td></tr>
<tr><td>Synchronization Functions</td><td>✔️</td><td></td></tr>
<tr><td>Mathematical Functions</td><td>🟨</td><td>Less common functions like native f16 math are not supported</td></tr>
<tr><td>Texture Functions</td><td>❌</td><td></td></tr>
<tr><td>Surface Functions</td><td>❌</td><td></td></tr>
<tr><td>Read-Only Data Cache Load Function</td><td>❌</td><td>No real need, immutable references hint this automatically</td></tr>
<tr><td>Load Functions Using Cache Hints</td><td>❌</td><td></td></tr>
<tr><td>Store Functions Using Cache Hints</td><td>❌</td><td></td></tr>
<tr><td>Time Function</td><td>✔️</td><td></td></tr>
<tr><td>Atomic Functions</td><td>❌</td><td></td></tr>
<tr><td>Address Space Predicate Functions</td><td>✔️</td><td>Address Spaces are implicitly handled, but they may be added for exotic interop with CUDA C/C++</td></tr>
<tr><td>Address Space Conversion Functions</td><td>✔️</td><td></td></tr>
<tr><td>Alloca Function</td><td>➖</td><td></td></tr>
<tr><td>Compiler Optimization Hint Functions</td><td>➖</td><td>Existing <code>core</code> hints work</td></tr>
<tr><td>Warp Vote Functions</td><td>❌</td><td></td></tr>
<tr><td>Warp Match Functions</td><td>❌</td><td></td></tr>
<tr><td>Warp Reduce Functions</td><td>❌</td><td></td></tr>
<tr><td>Warp Shuffle Functions</td><td>❌</td><td></td></tr>
<tr><td>Nanosleep</td><td>✔️</td><td></td></tr>
<tr><td>Warp Matrix Functions (Tensor Cores)</td><td>❌</td><td></td></tr>
<tr><td>Asynchronous Barrier</td><td>❌</td><td></td></tr>
<tr><td>Asynchronous Data Copies</td><td>❌</td><td></td></tr>
<tr><td>Profiler Counter Function</td><td>✔️</td><td></td></tr>
<tr><td>Assertion</td><td>✔️</td><td></td></tr>
<tr><td>Trap Function</td><td>✔️</td><td></td></tr>
<tr><td>Breakpoint</td><td>✔️</td><td></td></tr>
<tr><td>Formatted Output</td><td>✔️</td><td></td></tr>
<tr><td>Dynamic Global Memory Allocation</td><td>✔️</td><td></td></tr>
<tr><td>Execution Configuration</td><td>✔️</td><td></td></tr>
<tr><td>Launch Bounds</td><td>❌</td><td></td></tr>
<tr><td>Pragma Unroll</td><td>❌</td><td></td></tr>
<tr><td>SIMD Video Instructions</td><td>❌</td><td></td></tr>
<tr><td>Cooperative Groups</td><td>❌</td><td></td></tr>
<tr><td>Dynamic Parallelism</td><td>❌</td><td></td></tr>
<tr><td>Stream Ordered Memory</td><td>✔️</td><td></td></tr>
<tr><td>Graph Memory Nodes</td><td>❌</td><td></td></tr>
<tr><td>Unified Memory</td><td>✔️</td><td></td></tr>
<tr><td><code>__restrict__</code></td><td>➖</td><td>Not needed, you get that performance boost automatically through rust's noalias :)</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="frequently-asked-questions"><a class="header" href="#frequently-asked-questions">Frequently Asked Questions</a></h1>
<p>This page will cover a lot of the questions people often have when they encounter this project,
so they are addressed all at once.</p>
<h2 id="why-not-use-rustc-with-the-llvm-ptx-backend"><a class="header" href="#why-not-use-rustc-with-the-llvm-ptx-backend">Why not use rustc with the LLVM PTX backend?</a></h2>
<p>Good question, a good amount of reasons:</p>
<ul>
<li>The LLVM PTX backend is still very much WIP and often doesn't have things and/or breaks.</li>
<li>Due to odd dylib issues, the LLVM PTX backend does not work on windows, it will fail to link in intrinsics. 
This can be circumvented by building LLVM in a special way, but this is far beyond what most users will do to get a backend to work.</li>
<li>NVVM is used in NVCC itself, therefore NVIDIA is much more receptive to bugs inside of it. </li>
<li>NVVM contains proprietary optimizations (which is why it's closed source) that are simply not present in the LLVM PTX backend
which yield considerable performance differences (especially on more complex kernels with more information in the IR).</li>
<li>For some reason (either rustc giving weird LLVM IR or the LLVM PTX backend being broken) the LLVM PTX backend often
generates completely invalid PTX for trivial programs, so it is not an acceptable workflow for a production pipeline.</li>
<li>GPU and CPU codegen is fundamentally different, creating a codegen that is only for the GPU allows us to 
seamlessly implement features which would have been impossible or very difficult to implement in the existing codegen, such as:
<ul>
<li>Shared memory, this requires some special generation of globals with custom addrspaces, its just not possible to do without backend explicit handling.</li>
<li>Custom linking logic to do dead code elimination so as to not end up with large PTX files full of dead functions/globals.</li>
<li>Stripping away everything we do not need, no complex ABI handling, no shared lib handling, control over how function calls are generated, etc.</li>
</ul>
</li>
</ul>
<p>So overall, the LLVM PTX backend is fit for smaller kernels/projects/proofs of concept.
It is however not fit for compiling an entire language (core is <strong>very</strong> big) with dependencies and more. The end goal is for rust to be able to be used 
over CUDA C/C++ with the same (or better!) performance and features, therefore, we must take advantage of all optimizations NVCC has over us.</p>
<h2 id="if-nvvm-ir-is-a-subset-of-llvm-ir-can-we-not-give-rustc-generated-llvm-ir-to-nvvm"><a class="header" href="#if-nvvm-ir-is-a-subset-of-llvm-ir-can-we-not-give-rustc-generated-llvm-ir-to-nvvm">If NVVM IR is a subset of LLVM IR, can we not give rustc-generated LLVM IR to NVVM?</a></h2>
<p>Short answer, no.</p>
<p>Long answer, there are a couple of things that make this impossible:</p>
<ul>
<li>At the time of writing, libnvvm expects LLVM 7 bitcode, giving it LLVM 12/13 bitcode (which is what rustc uses) does not work.</li>
<li>NVVM IR is a <strong>subset</strong> of LLVM IR, there are tons of things that nvvm will not accept. Such as a lot of function attrs not being allowed. 
This is well documented and you can find the spec <a href="https://docs.nvidia.com/cuda/nvvm-ir-spec/index.html">here</a>. Not to mention
many bugs in libnvvm that i have found along the way, the most infuriating of which is nvvm not accepting integer types that arent <code>i1, i8, i16, i32, or i64</code>.
This required special handling in the codegen to convert these &quot;irregular&quot; types into vector types.</li>
</ul>
<h2 id="what-is-the-point-of-using-rust-if-a-lot-of-things-in-kernels-are-unsafe"><a class="header" href="#what-is-the-point-of-using-rust-if-a-lot-of-things-in-kernels-are-unsafe">What is the point of using Rust if a lot of things in kernels are unsafe?</a></h2>
<p>This is probably the most asked question by far, so let's break it down in detail. </p>
<p>TL;DR There are things we fundamentally can't check, but just because that is the case does not mean
we cannot still prevent a lot of problems we <em>can</em> check.</p>
<p>Yes it is true that GPU kernels have much more unsafe than CPU code usually, but why is that?</p>
<p>The reason is that CUDA's entire model is not based on safety in any way, there are almost zero
safety nets in CUDA. Rust is the polar opposite of this model, everything is safe unless there 
are some invariants that cannot be checked by the compiler. Let's take a look at some of the
invariants we face here.</p>
<p>Take this program as an example, written in CUDA C++:</p>
<pre><code class="language-cpp">__global__ void kernel(int* buf, int* other)
{
  int idx = threadIdx.x;
  buf[idx] = other[idx];
}

int main(void)
{
  int N = 50;
  int* a, b, d_a, d_b;
  a = (int*)malloc(N*sizeof(int));
  b = (int*)malloc(N*sizeof(int));

  cudaMalloc(&amp;d_a, N*sizeof(int));
  cudaMalloc(&amp;d_b, N*sizeof(int));

  for (int i = 0; i &lt; N; i++) {
    a[i] = 0.0f;
    b[i] = 2.0f;
  }

  cudaMemcpy(d_a, a, N*sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(d_b, b, N*sizeof(float), cudaMemcpyHostToDevice);

  kernel&lt;&lt;&lt;1, N&gt;&gt;&gt;(d_a, d_b);

  cudaMemcpy(d_a, a, N*sizeof(float), cudaMemcpyDeviceToHost);
  cudaMemcpy(d_b, b, N*sizeof(float), cudaMemcpyDeviceToHost);

  /* do something with the data */

  cudaFree(d_a);
  cudaFree(d_b);
  free(a);
  free(b);
}
</code></pre>
<p>You may think this looks innocent enough, it's a very easy and understandable program. But
if you really think about it, this is a minefield of things that could go wrong. Let's list most of them:</p>
<ul>
<li><code>buf</code> could be too small, that is undefined behavior (reading beyond allocated memory)</li>
<li>similarly, <code>other</code> could also be too small.</li>
<li>The kernel could have been called with too many or not enough parameters.</li>
<li>The kernel could have been called with a different grid/block dimension than expected, which would cause a data race.</li>
<li>Any of the <code>cudaMalloc</code>, <code>cudaMemcpy</code>, kernel launches, or <code>cudaFree</code> calls could have errored, which we dont handle and simply ignore.</li>
<li>We could have forgotten to initialize the buffers.</li>
<li>We could have forgotten to free the buffers.</li>
</ul>
<p>This goes to show that CUDA C/C++ and CUDA overall rely on shifting the burden of correctness from the API to the developer.
However, Rust uses a completely opposite design model, the compiler verifies as much as it can, and burden is only shifted to the 
developer if its absolutely essential, behind <code>unsafe</code>.</p>
<p>This creates a big problem for us, it is very difficult (and sometimes impossible) to prove correctness statically when wrapping 
how CUDA works. We can solve a lot of the points using things like RAII and providing a high level wrapper, but we fundamentally
cannot prove a lot of things, the most common place where this is shown is the CPU-GPU boundary, e.g. launching kernels.</p>
<p>Firstly, we cannot verify that the PTX we are calling is sound, that it has no data races, writes into the right buffers, doesnt rely
on undocumented invariants, and does not write invalid data to buffers. This already makes launching kernels perma-unsafe.</p>
<p>Second, CUDA does zero validation in terms of kernel parameter mismatch, it will simply segfault on you, or even keep going 
but produce invalid data (or cause the kernel to cause undefined behavior). This is a design flaw in CUDA itself, we have 
no control over it and no 100% reliable way to fix it, therefore we must shift this burden of correctness to the developer. </p>
<p>Moreover, the CUDA GPU kernel model is entirely based on trust, trusting each thread to index into the correct place in buffers,
trusting the caller of the kernel to uphold some dimension invariants, etc. This is once again, completely incompatible with how 
rust does things. We can provide wrappers to calculate an index that always works, and macros to index a buffer automatically, but 
indexing in complex ways is a core operation in CUDA and it is impossible for us to prove that whatever the developer is doing is correct.</p>
<p>Finally, We would love to be able to use mut refs in kernel parameters, but this is would be unsound. Because
each kernel function is <em>technically</em> called multiple times in parallel with the same parameters, we would be
aliasing the mutable ref, which Rustc declares as unsound (aliasing mechanics). So raw pointers or slightly-less-unsafe
need to be used. However, they are usually only used for the initial buffer indexing, after which you can turn them into a
mutable reference just fine (because you indexed in a way where no other thread will index that element). Also note
that shared refs can be used as parameters just fine.</p>
<p>Now that we outlined why this is a thing, why is using rust a benefit if we still need to use unsafe?</p>
<p>Well it's simple, eliminating most of the things that a developer needs to think about to have a safe program
is still exponentially safer than leaving <strong>everything</strong> to the developer to think about. </p>
<p>By using rust, we eliminate:</p>
<ul>
<li>The forgotten/unhandled CUDA errors problem (yay results!).</li>
<li>The uninitialized memory problem.</li>
<li>The forgetting to dealloc memory problem.</li>
<li>All of the inherent C++ problems in the kernel beyond the initial buffer indexing.</li>
<li>The mismatched grid/block dimension problem (by providing <code>thread::index</code>).</li>
<li>The forgetting to memcpy data back problem.</li>
</ul>
<p>And countless other problems with things like graphs, streams, devices, etc.</p>
<p>So, just because we cannot solve <em>every</em> problem with CUDA safety, does not mean we cannot solve 
a lot of them, and ease the burden of correctness from the developer. </p>
<p>Besides, using Rust only adds to safety, it does not make CUDA <em>more</em> unsafe. This means there are only
things to gain in terms of safety using Rust.</p>
<h2 id="why-not-use-rust-gpu-with-compute-shaders"><a class="header" href="#why-not-use-rust-gpu-with-compute-shaders">Why not use rust-gpu with compute shaders?</a></h2>
<p>The reasoning for this is the same reasoning as to why you would use CUDA over opengl/vulkan compute shaders:</p>
<ul>
<li>CUDA usually outperforms shaders if kernels are written well and launch configurations are optimal.</li>
<li>CUDA has many useful features such as shared memory, unified memory, graphs, fine grained thread control, streams, the PTX ISA, etc.</li>
<li>rust-gpu does not perform many optimizations, and with cg_ssa's less than ideal codegen, the optimizations by llvm and libnvvm are needed.</li>
<li>SPIRV is arguably still not suitable for serious GPU kernel codegen, it is underspecced, complex, and does not mention many things which are needed.
While libnvvm (which uses a well documented subset of LLVM IR) and the PTX ISA are very thoroughly documented/specified.</li>
<li>rust-gpu is primarily focused on graphical shaders, compute shaders are secondary, which the rust ecosystem needs, but it also 
needs a project 100% focused on computing, and computing only.</li>
<li>SPIRV cannot access many useful CUDA libraries such as Optix, cuDNN, cuBLAS, etc.</li>
<li>SPIRV debug info is still very young and rust-gpu cannot generate it. While rustc_codegen_nvvm does, which can be used
for profiling kernels in something like nsight compute.</li>
</ul>
<p>Moreover, CUDA is the primary tool used in big computing industries such as VFX and scientific computing. Therefore 
it is much easier for CUDA C++ users to use rust for GPU computing if most of the concepts are still the same. Plus,
we can interface with existing CUDA code by compiling it to PTX then linking it with our rust code using the CUDA linker
API (which is exposed in a high level wrapper in cust).</p>
<h2 id="why-use-the-cuda-driver-api-over-the-runtime-api"><a class="header" href="#why-use-the-cuda-driver-api-over-the-runtime-api">Why use the CUDA Driver API over the Runtime API?</a></h2>
<p>Simply put, the driver API provides better control over concurrency, context, and module management, and overall has better performance 
control than the runtime API.</p>
<p>Let's break it down into the main new concepts introduced in the Driver API.</p>
<h3 id="contexts"><a class="header" href="#contexts">Contexts</a></h3>
<p>The first big difference in the driver API is that CUDA context management is explicit and not implicit.</p>
<p>Contexts are similar to CPU processes, they manage all of the resources, streams, allocations, etc associated with
operations done inside them. </p>
<p>The driver API provides control over these contexts. You can create new contexts and drop them at any time. 
As opposed to the runtime API which works off of an implicit context destroyed on device reset. This
causes a problem for larger applications because a new integration of CUDA could call device reset
when it is finished, which causes further uses of CUDA to fail.</p>
<h3 id="modules"><a class="header" href="#modules">Modules</a></h3>
<p>Modules are the second big difference in the driver API. Modules are similar to shared libraries, they
contain all of the globals and functions (kernels) inside of a PTX/cubin file. The driver API
is language-agnostic, it purely works off of ptx/cubin files. To answer why this is important we
need to cover what cubins and ptx files are briefly.</p>
<p>PTX is a low level assembly-like language which is the penultimate step before what the GPU actually
executes. It is human-readable and you can dump it from a CUDA C++ program with <code>nvcc ./file.cu --ptx</code>.
This PTX is then optimized and lowered into a final format called SASS (Source and Assembly) and 
turned into a cubin (CUDA binary) file. </p>
<p>Driver API modules can be loaded as either ptx, cubin, or fatbin files. If they are loaded as 
ptx then the driver API will JIT compile the PTX to cubin then cache it. You can also
compile ptx to cubin yourself using ptx-compiler and cache it.</p>
<p>This pipeline provides much better control over what functions you actually need to load and cache.
You can separate different functions into different modules you can load dynamically (and even dynamically reload).
This can yield considerable performance benefits when dealing with a lot of functions.</p>
<h3 id="streams"><a class="header" href="#streams">Streams</a></h3>
<p>Streams are (one of) CUDA's way of dispatching multiple kernels in parallel. You can kind of think of them 
as OS threads essentially. Kernels dispatched one after the other inside of a particular stream
will execute one after the other on the GPU, which is helpful for kernels that rely on a previous kernel's result.</p>
<p>The CUDA runtime API operates off of a single global stream. This causes a lot of issues for users of large programs or libraries that
need to manage many kernels being dispatched at the same time as efficiently as possible.</p>
<h2 id="why-target-nvidia-gpus-only-instead-of-using-something-that-can-work-on-amd"><a class="header" href="#why-target-nvidia-gpus-only-instead-of-using-something-that-can-work-on-amd">Why target NVIDIA GPUs only instead of using something that can work on AMD?</a></h2>
<p>This is a complex issue with many arguments for both sides, so i will give you
both sides as well as my opinion. </p>
<p>Pros for using OpenCL over CUDA:</p>
<ul>
<li>OpenCL (mostly) works on everything because it is a specification, not an actual centralized tool.</li>
<li>OpenCL will be decently fast on most systems.</li>
</ul>
<p>Cons for using OpenCL over CUDA:</p>
<ul>
<li>Just like all open specifications, not every implementation is as good or supports the same things.
Just because the absolute basics work, does not mean more exotic features work on everything because
some vendors may lag behind others.</li>
<li>OpenCL is slow to add new features, this is a natural consequence of being an open specification many
vendors need to implement. For example, OpenCL 3.0 (which was announced in around April 2020) is supported
by basically nobody. NVIDIA cards support OpenCL 2.0 while AMD cards support OpenCL 2.1. This means
new features cannot be reliably relied upon because they are unlikely to work on a lot of cards for a LONG time.</li>
<li>OpenCL can only be written in OpenCL C (based on C99), OpenCL C++ is a thing, but again, not everything
supports it. This makes complex programs more difficult to create.</li>
<li>OpenCL has less tools and libraries.</li>
<li>OpenCL is nowhere near as language-agnostic as CUDA. CUDA works almost fully off of an assembly format (ptx)
and debug info. Essentially how CPU code works. This makes writing language-agnostic things in OpenCL near impossible and
locks you into using OpenCL C.</li>
<li>OpenCL is plagued with serious driver bugs which have not been fixed, or that occur only on certain vendors.</li>
</ul>
<p>Pros for using CUDA over OpenCL:</p>
<ul>
<li>CUDA is for the most part the industry-standard tool for &quot;higher level&quot; computing such as scientific or
VFX computing.</li>
<li>CUDA is a proprietary tool, meaning that NVIDIA is able to push out bug fixes and features much faster
than releasing a new spec and waiting for vendors to implement it. This allows for more features being added, 
such as cooperative kernels, cuda graphs, unified memory, new profilers, etc.</li>
<li>CUDA is a single entity, meaning that if something does or does not work on one system it is unlikely 
that that will be different on another system. Assuming you are not using different architectures, where
one gpu may be lacking a feature.</li>
<li>CUDA is usually 10-30% faster than OpenCL overall, this is likely due to subpar OpenCL drivers by NVIDIA,
but it is unlikely this performance gap will change in the near future.</li>
<li>CUDA has a much richer set of libraries and tools than OpenCL, such as cuFFT, cuBLAS, cuRand, cuDNN, OptiX, NSight Compute, cuFile, etc.</li>
<li>You can seamlessly use existing CUDA C/C++ code with <code>cust</code> or <code>rustc_codegen_nvvm</code>-generated PTX by
using the CUDA linker APIs which are exposed in <code>cust</code>. Allowing for incremental switching to Rust.</li>
<li>There is a generally larger set of code samples in CUDA C/C++ over OpenCL.</li>
<li>Documentation is <strong>far</strong> better, there are (mostly) complete API docs for every single CUDA library and function out there.</li>
<li>CUDA generally offers more control over the internals of how CUDA executes your GPU code. For example, you can choose
to keep PTX which uses a virtual architecture, or you can compile that to cubin (SASS) and cache that for faster load times.</li>
</ul>
<p>Cons for using CUDA over OpenCL:</p>
<ul>
<li>CUDA only works on NVIDIA GPUs.</li>
</ul>
<h1 id="what-makes-cust-and-rustacuda-different"><a class="header" href="#what-makes-cust-and-rustacuda-different">What makes cust and RustaCUDA different?</a></h1>
<p>Cust is a fork of rustacuda which changes a lot of things inside of it, as well as adds new features that
are not inside of rustacuda. </p>
<p>The most significant changes (This list is not complete!!) are:</p>
<ul>
<li>Drop code no longer panics on failure to drop raw CUDA handles, this is so that InvalidAddress errors, which cause 
CUDA to nuke the driver and nuke any memory allocations no longer cause piles of panics from device boxes trying to be 
dropped when returning from the function with <code>?</code>.</li>
<li>cuda-sys is no longer used, instead, we have our own bindings <code>cust_raw</code> so we can ensure updates to the latest CUDA features.</li>
<li>CUDA occupancy functions have been added.</li>
<li>PTX linking functions have been added.</li>
<li>Native support for <code>vek</code> linear algebra types for grid/block dimensions and DeviceCopy has been added under the <code>vek</code> feature.</li>
<li>Util traits have been added.</li>
<li>Basic graph support has been added.</li>
<li>Some functions have been renamed.</li>
<li>Some functions have been added.</li>
</ul>
<p>Changes that are currently in progress but not done/experimental:</p>
<ul>
<li>Surfaces</li>
<li>Textures</li>
<li>Graphs</li>
<li>PTX validation</li>
</ul>
<p>Just like rustacuda, cust makes no assumptions of what language was used to generate the ptx/cubin. It could be 
C, C++, futhark, or best of all, Rust!</p>
<p>Cust's name is literally just rust + cuda mashed together in a horrible way.
Or you can pretend it stands for custard if you really like custard.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="guide"><a class="header" href="#guide">Guide</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h1>
<p>This section covers how to get started writing GPU crates with <code>cuda_std</code> and <code>cuda_builder</code>.</p>
<h2 id="required-libraries"><a class="header" href="#required-libraries">Required Libraries</a></h2>
<p>Before you can use the project to write GPU crates, you will need a couple of prerequisites:</p>
<ul>
<li>
<p><a href="https://developer.nvidia.com/cuda-downloads">The CUDA SDK</a>, version <code>11.2</code> or higher (and the appropriate driver - <a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html">see cuda release notes</a>) . This is only for building
GPU crates, to execute built PTX you only need CUDA 9+.</p>
</li>
<li>
<p>LLVM 7.x (7.0 to 7.4), The codegen searches multiple places for LLVM:</p>
<ul>
<li>If <code>LLVM_CONFIG</code> is present, it will use that path as <code>llvm-config</code>.</li>
<li>Or, if <code>llvm-config</code> is present as a binary, it will use that, assuming that <code>llvm-config --version</code> returns <code>7.x.x</code>.</li>
<li>Finally, if neither are present or unusable, it will attempt to download and use prebuilt LLVM. This currently only
works on Windows however.</li>
</ul>
</li>
<li>
<p>The OptiX SDK if using the optix library (the pathtracer example uses it for denoising).</p>
</li>
<li>
<p>You may also need to add <code>libnvvm</code> to PATH, the builder should do it for you but in case it does not work, add libnvvm to PATH, it should be somewhere like <code>CUDA_ROOT/nvvm/bin</code>,</p>
</li>
<li>
<p>You may wish to use or consult the bundled <a href="guide/getting_started.html#docker">Dockerfile</a> to assist in your local config</p>
</li>
</ul>
<h2 id="rust-toolchain"><a class="header" href="#rust-toolchain">rust-toolchain</a></h2>
<p>Currently, the Codegen only works on nightly (because it uses rustc internals), and it only works on a specific version of nightly.
This is why you must copy the <code>rust-toolchain</code> file in the project repository to your own project. This will ensure
you are on the correct nightly version so the codegen builds.</p>
<p>Only the codegen requires nightly, <code>cust</code> and other CPU-side libraries work perfectly fine on stable.</p>
<h2 id="cargotoml"><a class="header" href="#cargotoml">Cargo.toml</a></h2>
<p>Now we can actually get started creating our GPU crate 🎉</p>
<p>Start by making a normal crate as you normally would, manually or with <code>cargo init</code>: <code>cargo init name --lib</code>.</p>
<p>After this, we just need to add a couple of things to our Cargo.toml:</p>
<pre><code class="language-diff">[package]
name = &quot;name&quot;
version = &quot;0.1.0&quot;
edition = &quot;2021&quot;

+[lib]
+crate-type = [&quot;cdylib&quot;, &quot;rlib&quot;]

[dependencies]
+cuda_std = &quot;XX&quot;
</code></pre>
<p>Where <code>XX</code> is the latest version of <code>cuda_std</code>.</p>
<p>We changed our crate's crate types to <code>cdylib</code> and <code>rlib</code>. We specified <code>cdylib</code> because the nvptx targets do not support binary crate types.
<code>rlib</code> is so that we will be able to use the crate as a dependency, such as if we would like to use it on the CPU.</p>
<h2 id="librs"><a class="header" href="#librs">lib.rs</a></h2>
<p>Before we can write any GPU kernels, we must add a few directives to our <code>lib.rs</code> which are required by the codegen:</p>
<pre><code class="language-rs">#![cfg_attr(
    target_os = &quot;cuda&quot;,
    no_std,
    feature(register_attr),
    register_attr(nvvm_internal)
)]

use cuda_std::*;
</code></pre>
<p>This does a couple of things:</p>
<ul>
<li>It only applies the attributes if we are compiling the crate for the GPU (target_os = &quot;cuda&quot;).</li>
<li>It declares the crate to be <code>no_std</code> on CUDA targets.</li>
<li>It registers a special attribute required by the codegen for things like figuring out
what functions are GPU kernels.</li>
<li>It explicitly includes <code>kernel</code> macro and <code>thread</code></li>
</ul>
<p>If you would like to use <code>alloc</code> or things like printing from GPU kernels (which requires alloc) then you need to declare <code>alloc</code> too:</p>
<pre><code class="language-rs">extern crate alloc;
</code></pre>
<p>Finally, if you would like to use types such as slices or arrays inside of GPU kernels you must allow <code>improper_cytypes_definitions</code> either on the whole crate or the individual GPU kernels. This is because on the CPU, such types are not guaranteed to be passed a certain way, so they should not be used in <code>extern &quot;C&quot;</code> functions (which is what kernels are implicitly declared as). However, <code>rustc_codegen_nvvm</code> guarantees the way in which things like structs, slices, and arrays are passed. See <a href="guide/./kernel_abi.html">Kernel ABI</a>.</p>
<pre><code class="language-rs">#![allow(improper_ctypes_definitions)]
</code></pre>
<h2 id="writing-our-first-gpu-kernel"><a class="header" href="#writing-our-first-gpu-kernel">Writing our first GPU kernel</a></h2>
<p>Now we can finally start writing an actual GPU kernel. </p>
<details>
  <summary>Expand this section if you are not familiar with how GPU-side CUDA works</summary>
<p>Firstly, we must explain a couple of things about GPU kernels, specifically, how they are executed. GPU Kernels (functions) are the entry point for executing anything on the GPU, they are the functions which will be executed from the CPU. GPU kernels do not return anything, they write their data to buffers passed into them.</p>
<p>CUDA's execution model is very very complex and it is unrealistic to explain all of it in
this section, but the TLDR of it is that CUDA will execute the GPU kernel once on every
thread, with the number of threads being decided by the caller (the CPU).</p>
<p>We call these parameters the launch dimensions of the kernel. Launch dimensions are split
up into two basic concepts:</p>
<ul>
<li>Threads, a single thread executes the GPU kernel <strong>once</strong>, and it makes the index
of itself available to the kernel through special registers (functions in our case).</li>
<li>Blocks, Blocks house multiple threads that they execute on their own. Thread indices
are only unique across the thread's block, therefore CUDA also exposes the index
of the current block.</li>
</ul>
<p>One important thing to note is that block and thread dimensions may be 1d, 2d, or 3d.
That is to say, i can launch <code>1</code> block of <code>6x6x6</code>, <code>6x6</code>, or <code>6</code> threads. I could 
also launch <code>5x5x5</code> blocks. This is very useful for 2d/3d applications because it makes
the 2d/3d index calculations much simpler. CUDA exposes thread and block indices 
for each dimension through special registers. We expose thread index queries through
<code>cuda_std::thread</code>.</p>
</details>
<p>Now that we know how GPU functions work, let's write a simple kernel. We will write
a kernel which does <code>[1, 2, 3, 4] + [1, 2, 3, 4] = [2, 4, 6, 8]</code>. We will use 
a 1-dimensional index and use the <code>cuda_std::thread::index_1d</code> utility method to 
calculate a globally-unique thread index for us (this index is only unique if the kernel was launched with a 1d launch config!).</p>
<pre><code class="language-rs">#[kernel]
pub unsafe fn add(a: &amp;[f32], b: &amp;[f32], c: *mut f32) {
    let idx = thread::index_1d() as usize;
    if idx &lt; a.len() {
        let elem = &amp;mut *c.add(idx);
        *elem = a[idx] + b[idx];
    }
}
</code></pre>
<p>If you have used CUDA C++ before, this should seem fairly familiar, with a few oddities:</p>
<ul>
<li>Kernel functions must be unsafe currently, this is because the semantics of Rust safety 
on the GPU are still very much undecided. This restriction will probably be removed in the future.</li>
<li>We use <code>*mut f32</code> and not <code>&amp;mut [f32]</code>. This is because using <code>&amp;mut</code> in function arguments
is unsound. The reason being that rustc assumes <code>&amp;mut</code> does not alias. However, because every thread gets a copy of the arguments, this would cause it to alias, thereby violating
this invariant and yielding technically unsound code. Pointers do not have such an invariant on the other hand. Therefore, we use a pointer and only make a mutable reference once we 
are sure the elements are disjoint: <code>let elem = &amp;mut *c.add(idx);</code>.</li>
<li>We check that the index is not out of bounds before doing anything, this is because it is
common to launch kernels with thread amounts that are not exactly divisible by the length for optimization.</li>
</ul>
<p>Internally what this does is it first checks that a couple of things are right in the kernel:</p>
<ul>
<li>All parameters are <code>Copy</code>.</li>
<li>The function is <code>unsafe</code>.</li>
<li>The function does not return anything.</li>
</ul>
<p>Then it declares this kernel to the codegen so that the codegen can tell CUDA this is a GPU kernel.
It also applies <code>#[no_mangle]</code> so the name of the kernel is the same as it is declared in the code.</p>
<h2 id="building-the-gpu-crate"><a class="header" href="#building-the-gpu-crate">Building the GPU crate</a></h2>
<p>Now that you have some kernels defined in a crate, you can build them easily using <code>cuda_builder</code>.
<code>cuda_builder</code> is a helper crate similar to <code>spirv_builder</code> (if you have used rust-gpu before), it builds
GPU crates while passing everything needed by rustc.</p>
<p>To use it you can simply add it as a build dependency in your CPU crate (the crate running the GPU kernels):</p>
<pre><code class="language-diff">+[build-dependencies]
+cuda_builder = &quot;XX&quot;
</code></pre>
<p>Where <code>XX</code> is the current version of cuda_builder.</p>
<p>Then, you can simply invoke it in the build.rs of your CPU crate:</p>
<pre><code class="language-rs">use cuda_builder::CudaBuilder;

fn main() {
    CudaBuilder::new(&quot;path/to/gpu/crate/root&quot;)
        .copy_to(&quot;some/path.ptx&quot;)
        .build()
        .unwrap();
}
</code></pre>
<p>The first argument is the path to the root of the GPU crate you are trying to build, which would probably be <code>../name</code> in our case.
The second function <code>.copy_to(path)</code> tells the builder to copy the built PTX file somewhere. By default the builder puts the PTX file 
inside of <code>target/cuda-builder/nvptx64-nvidia-cuda/release/crate_name.ptx</code>, but it is usually helpful to copy it to another path, which is
what such method does. Finally, <code>build()</code> actually runs rustc to compile the crate. This may take a while since it needs to build things like core
from scratch, but after the first compile, incremental will make it much faster.</p>
<p>Finally, you can include the PTX as a static string in your program:</p>
<pre><code class="language-rs">static PTX: &amp;str = include_str!(&quot;some/path.ptx&quot;);
</code></pre>
<p>Then execute it using cust.</p>
<p>Don't forget to include the current <code>rust-toolchain</code> in the top of your project:</p>
<pre><code class="language-toml"># If you see this, run `rustup self update` to get rustup 1.23 or newer.

# NOTE: above comment is for older `rustup` (before TOML support was added),
# which will treat the first line as the toolchain name, and therefore show it
# to the user in the error, instead of &quot;error: invalid channel name '[toolchain]'&quot;.

[toolchain]
channel = &quot;nightly-2021-12-04&quot;
components = [&quot;rust-src&quot;, &quot;rustc-dev&quot;, &quot;llvm-tools-preview&quot;]
</code></pre>
<h2 id="docker"><a class="header" href="#docker">Docker</a></h2>
<p>There is also a <a href="guide/Dockerfile">Dockerfile</a> prepared as a quickstart with all the necessary libraries for base cuda development.</p>
<p>You can use it as follows (assuming your clone of Rust-CUDA is at the absolute path <code>RUST_CUDA</code>):</p>
<ul>
<li>Ensure you have Docker setup to <a href="https://docs.docker.com/config/containers/resource_constraints/#gpu">use gpus</a></li>
<li>Build <code>docker build -t rust-cuda $RUST_CUDA</code></li>
<li>Run <code>docker run -it --gpus all -v $RUST_CUDA:/root/rust-cuda --entrypoint /bin/bash rust-cuda</code>
<ul>
<li>Running will drop you into the container's shell and you will find the project at <code>~/rust-cuda</code></li>
</ul>
</li>
<li>If all is well, you'll be able to <code>cargo run</code> in <code>~/rust-cuda/examples/cuda/cpu/add</code></li>
</ul>
<p><strong>Notes:</strong></p>
<ol>
<li>refer to <a href="guide/getting_started.html#rust-toolchain">rust-toolchain</a> to ensure you are using the correct toolchain in your project.</li>
<li>despite using Docker, your machine will still need to be running a compatible driver, in this case for Cuda 11.4.1 it is &gt;=470.57.02</li>
<li>if you have issues within the container, it can help to start ensuring your gpu is recognized
<ul>
<li>ensure <code>nvidia-smi</code> provides meaningful output in the container</li>
<li>NVidia provides a number of samples https://github.com/NVIDIA/cuda-samples. In particular, you may want to try <code>make</code>ing and running the <a href="https://github.com/NVIDIA/cuda-samples/tree/ba04faaf7328dbcc87bfc9acaf17f951ee5ddcf3/Samples/deviceQuery"><code>deviceQuery</code></a> sample. If all is well you should see many details about your gpu</li>
</ul>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tips"><a class="header" href="#tips">Tips</a></h1>
<p>This section contains some tips on what to do and what not to do using the project.</p>
<h2 id="gpu-kernels"><a class="header" href="#gpu-kernels">GPU kernels</a></h2>
<ul>
<li>
<p>Generally don't derive <code>Debug</code> for structs in GPU crates. The codegen currently does not do much global
DCE (dead code elimination) so debug can really slow down compile times and make the PTX gigantic. This
will get much better in the future but currently it will cause some undesirable effects.</p>
</li>
<li>
<p>Don't use recursion, CUDA allows it but threads have very limited stacks (local memory) and stack overflows
yield confusing <code>InvalidAddress</code> errors. If you are getting such an error, run the executable in cuda-memcheck,
it should yield a write failure to <code>Local</code> memory at an address of about 16mb. You can also put the ptx file through
<code>cuobjdump</code> and it should yield ptxas warnings for functions without a statically known stack usage.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kernel-abi"><a class="header" href="#kernel-abi">Kernel ABI</a></h1>
<p>This section details how parameters are passed to GPU kernels by the Codegen at the current time. 
In other words, how the codegen expects you to pass different types to GPU kernels from the CPU.</p>
<p>⚠️ If you find any bugs in the ABI please report them. ⚠️</p>
<h2 id="preface"><a class="header" href="#preface">Preface</a></h2>
<p>Please note that the following <strong>only</strong> applies to non-rust call conventions, we make zero guarantees 
about the rust call convention, just like rustc. </p>
<p>While we currently override every ABI except rust, you should generally only use <code>&quot;C&quot;</code>, any 
other ABI we override purely to avoid footguns.</p>
<p>Functions marked as <code>#[kernel]</code> are enforced to be <code>extern &quot;C&quot;</code> by the kernel macro, and it is expected
that <strong>all</strong> GPU kernels be <code>extern &quot;C&quot;</code>, not that you should be declaring any kernels without the <code>#[kernel]</code> macro,
because the codegen/cuda_std is allowed to rely on the behavior of <code>#[kernel]</code> for correctness.</p>
<h2 id="structs"><a class="header" href="#structs">Structs</a></h2>
<p>Structs are always passed directly using byte arrays if they are passed by value in the function. This
corresponds to what is expected by CUDA/the PTX ABI.</p>
<p>For example:</p>
<pre><code class="language-rs">#[derive(Clone, Copy)]
#[repr(C)]
pub struct Foo {
    pub a: u16,
    pub b: u64,
    pub c: u128,
}

#[kernel]
pub unsafe fn kernel(a: Foo) {
    /* ... */
}
</code></pre>
<p>will map to the following PTX:</p>
<pre><code>.visible .entry kernel(
	.param .align 16 .b8 kernel_param_0[32]
)
</code></pre>
<p>Consequently, it is expected that you will pass the struct by value when launching the kernel, and not
by reference (by allocating a device box):</p>
<pre><code class="language-rs">let foo = Foo { 
  a: 5,
  b: 6,
  c: 7
};

unsafe {
  launch!(
    module.kernel&lt;&lt;&lt;1, 1, 0, stream&gt;&gt;&gt;(foo)
  )?;
}
</code></pre>
<p>And not</p>
<pre><code class="language-rs">let foo = DeviceBox::new(Foo { 
  a: 5,
  b: 6,
  c: 7
});

unsafe {
  launch!(
    module.kernel&lt;&lt;&lt;1, 1, 0, stream&gt;&gt;&gt;(foo.as_device_ptr())
  )?;
}
</code></pre>
<h2 id="arrays"><a class="header" href="#arrays">Arrays</a></h2>
<p>Arrays are passed the same as if they were structs, they are always passed by value as byte arrays.</p>
<h2 id="slices"><a class="header" href="#slices">Slices</a></h2>
<p>Slices are passed as <strong>two parameters</strong>, both 32-bit on <code>nvptx</code> or 64-bit on <code>nvptx64</code>. The first parameter is the pointer
to the beginning of the data, and the second parameter is the length of the slice.</p>
<p>For example:</p>
<pre><code class="language-rs">#[kernel]
pub unsafe fn kernel(a: &amp;[u8]) {
  /* ... */
}
</code></pre>
<p>Will map to the following PTX (on nvptx64):</p>
<pre><code>.visible .entry kernel(
	.param .u64 kernel_param_0,
	.param .u64 kernel_param_1
)
</code></pre>
<p>Consequently, it is expected that you will pass the pointer and the length as multiple parameters when calling the kernel:</p>
<pre><code class="language-rs">let mut buf = [5u8; 10].as_dbuf()?;

unsafe {
  launch!(
    module.kernel&lt;&lt;&lt;1, 1, 0, stream&gt;&gt;&gt;(buf.as_device_ptr(), buf.len())
  )?;
}
</code></pre>
<p>You may get warnings about slices being an improper C-type, but the warnings are safe to ignore, the codegen guarantees 
that slices are passed as pairs of params.</p>
<p>You cannot however pass mutable slices, this is because it would violate aliasing rules, each thread receiving a copy of the mutable
slice would violate aliasing rules. You may use a <code>&amp;[UnsafeCell&lt;T&gt;]</code> then convert an element to a mutable ref (once you know the element accesses
are disjoint), or more commonly, use a raw pointer.</p>
<h2 id="zsts"><a class="header" href="#zsts">ZSTs</a></h2>
<p>ZSTs (zero-sized types) are ignored and become nothing in the final PTX.</p>
<h2 id="primitives"><a class="header" href="#primitives">Primitives</a></h2>
<p>Primitive types are passed directly by value, same as structs. They map to the special PTX types <code>.s8</code>, <code>.s16</code>, <code>.s32</code>, <code>.s64</code>, <code>.u8</code>, <code>.u16</code>, <code>.u32</code>, <code>.u64</code>, <code>.f32</code>, and <code>.f64</code>.
With the exception that <code>u128</code> and <code>i128</code> are passed as byte arrays (but this has no impact on how they are passed from the CPU).</p>
<h2 id="references-and-pointers"><a class="header" href="#references-and-pointers">References And Pointers</a></h2>
<p>References and Pointers are both passed as expected, as pointers. It is therefore expected that you pass such parameters using device memory:</p>
<pre><code class="language-rs">#[kernel]
pub unsafe fn kernel(a: &amp;u8) {
  /* ... */
}
</code></pre>
<pre><code class="language-rs">let mut val = DeviceBox::new(&amp;5)?;

unsafe {
  launch!(
    module.kernel&lt;&lt;&lt;1, 1, 0, stream&gt;&gt;&gt;(val.as_device_ptr())
  )?;
}
</code></pre>
<h2 id="reprrust-types"><a class="header" href="#reprrust-types">repr(Rust) Types</a></h2>
<p>using repr(Rust) types inside of kernels is not disallowed but it is highly discouraged. This is because rustc is allowed to switch up
how the types are represented across compiler invocations which leads to hard to track errors.</p>
<p>Therefore, you should generally only use repr(C) inside of kernel parameters. With the exception of slices that have a guaranteed parameter layout.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="safety"><a class="header" href="#safety">Safety</a></h1>
<p>With one of Rust's main foci being memory safety, we strive to make most things
safe, without requiring too much unsafe usage and mental checks from the user. However,
CUDA's inherent thread/memory model leaves many things ambiguous as to whether they are sound
and makes many invariants inherently impossible to statically prove. In this section
we will talk about what kinds of behavior is considered undefined inside of kernels as 
well as the invariants that must be upheld by the caller of kernels.</p>
<p>⚠️ This list is not fully complete, as the semantics of Rust safety on the GPU have not been explored much,
however, this list includes most of the actions &quot;regular&quot; users may commit ⚠️</p>
<h2 id="behavior-considered-undefined-in-gpu-kernels"><a class="header" href="#behavior-considered-undefined-in-gpu-kernels">Behavior considered undefined in GPU kernels</a></h2>
<p>This list is purely meant to be a guide on what behavior should be avoided.</p>
<p>Undefined behavior on the GPU is defined as potentially being able to cause the following (but not limited to):</p>
<ul>
<li>Unknown/Undefined data being written to a location in memory.</li>
<li>Causing fatal termination of either just the kernel (through trapping), or the entire CUDA driver (through invalid address errors).</li>
<li>Causing LLVM/NVVM to optimize the code into unknown code.</li>
</ul>
<p>Behavior considered undefined inside of GPU kernels:</p>
<ul>
<li>Most importantly, any behavior that is considered undefined on the CPU, is considered undefined
on the GPU too. See: https://doc.rust-lang.org/reference/behavior-considered-undefined.html.
The only exception being invalid sizes for buffers given to a GPU kernel.</li>
</ul>
<p>Currently we declare that the invariant that a buffer given to a gpu kernel must be large enough for any access the
kernel is going to make is up to the caller of the kernel to uphold. This idiom may be changed in the future.</p>
<ul>
<li>Any kind of data race, this has the same semantics as data races in CPU code. Such as:
<ul>
<li>Multiple threads writing to a location in memory at the same time without synchronization.</li>
<li>One or more threads reading while a thread is writing to a memory location.</li>
<li>Reading shared memory while a thread is writing to the location (if for example <code>thread::sync</code> has not been called).</li>
</ul>
</li>
</ul>
<p>Behavior not currently considered undefined, but considered undesirable:</p>
<ul>
<li>calling <code>thread::sync</code> inside of a branch that not all threads inside of the thread block have reached.</li>
</ul>
<h2 id="behavior-considered-undefined-on-the-cpu"><a class="header" href="#behavior-considered-undefined-on-the-cpu">Behavior considered undefined on the CPU</a></h2>
<p>This list will contain behavior that is considered undefined in the context of actually launching GPU kernels from
the CPU.</p>
<h3 id="streams-1"><a class="header" href="#streams-1">Streams</a></h3>
<p>Streams will always execute concurrently with eachother. That is to say, kernels launched
inside of a single stream guarantee that they will be executed one after the other, in order.</p>
<p>However, kernels launched in different streams have no guarantee of execution order, their execution
may be interleaved and kernels are likely to be launched concurrently on the GPU.</p>
<p><img src="guide/../../assets/streams.svg" alt="" /></p>
<p>Therefore, it is undefined behavior to write to the same memory location in kernels executed in different
streams without synchronization.</p>
<p>For example:
1: <code>Foo</code> is allocated as a buffer of memory on the GPU.
2: Stream <code>1</code> launches kernel <code>bar</code> which writes to <code>Foo</code>.
3: Stream <code>2</code> launches kernel <code>bar</code> which also writes to <code>Foo</code>.</p>
<p>This is undefined behavior because the kernels are likely to be executed concurrently, causing a data
race when multiple kernels try to write to the same memory.</p>
<p>However, if the thread that Stream <code>2</code> is located on calls <code>synchronize()</code> on Stream <code>1</code> before launching the kernel,
this will be sound. Because <code>synchronize()</code> waits for Stream <code>2</code> to finish all of its tasks before giving back control
to the calling thread.</p>
<p>Another important detail is that GPU operations on a stream are NOT synchronized with the CPU. 
This means that CPU code may not rely on a kernel being finished without calling <code>synchronize()</code>. For example:</p>
<pre><code class="language-rs">launch!(module.bar&lt;&lt;&lt;1, 1, 0, stream&gt;&gt;&gt;(foo.as_unified_ptr()))?;
// 'bar' is not guaranteed to be finished executing at this point.
function_that_accesses_foo(foo);
stream.synchronize()?;
// foo may be accessed and will see the changes that 'bar' wrote to 'foo'. 'bar' is guaranteed 
// to be finished executing.
</code></pre>
<h3 id="contexts-1"><a class="header" href="#contexts-1">Contexts</a></h3>
<p>Contexts are akin to CPU processes, therefore, it is undefined behavior (although it should always yield an invalid address error) to
access another context's allocated GPU memory. </p>
<p>However, this is very uncommon because single-device code should not need multiple contexts generally. This only becomes relevant
when using multiple devices (multi-GPU code) with different contexts.</p>
<p>Note however, that unified memory can be accessed by multiple GPUs and multiple contexts at the same time, as unified memory
takes care of copying and moving data automatically from GPUs/CPU when a page fault occurs. For this reason
as well as general ease of use, we suggest that unified memory generally be used over regular device memory.</p>
<h3 id="kernel-launches"><a class="header" href="#kernel-launches">Kernel Launches</a></h3>
<p>Kernel Launches are the most unsafe part of CUDA, many things must be checked by the developer to soundly launch a kernel.
It is fundamentally impossible for us to verify a large portion of the invariants expected by the kernel/CUDA.</p>
<p>The following invariants must be upheld by the caller of a kernel, failure to do so is undefined behavior:</p>
<ul>
<li>The number of parameters passed to the kernel must match the expected number of parameters.</li>
<li>The dimensionality expected by the kernel must match, e.g. if the kernel expects 2d thread indices, it is undefined
behavior to launch the kernel with 3d thread indices (which would cause a data race). However, it is not undefined behavior
to launch the kernel with a dimensionality lower than expected, e.g. launching a 2d kernel with a 1d dimensionality.</li>
<li>The types expected by the kernel must match:
<ul>
<li>If the kernel expects a struct, if the struct is repr(Rust), the struct must be the actual struct from the kernel library,
otherwise, if it is repr(C) (which is reccomended), the fields must all match, including alignment and order of fields.</li>
</ul>
</li>
<li>Reference aliasing rules must not be violated, including:
<ul>
<li>Immutable references are allowed to be aliased, e.g. if a kernel expects <code>&amp;T</code> and <code>&amp;T</code>, it is sound to pass the same pointer for both.</li>
<li>Data behind an immutable reference must not be modified, meaning, it is undefined behavior to pass the same pointer to <code>&amp;T</code> and <code>*mut T</code>,
where <code>*mut T</code> is used for modifying the data. </li>
<li>Parameters such as <code>&amp;[UnsafeCell&lt;T&gt;]</code> must be exclusive, assuming the kernel uses the <code>UnsafeCell&lt;T&gt;</code> to modify the data.</li>
<li><code>*mut T</code> does not necessarily need to follow aliasing rules, it is sound to pass the same pointer to two <code>*mut T</code> parameters
assuming that the kernel accesses nonoverlapping regions of the memory. If a mutable reference is formed from
the pointer, the mutable reference <strong>must</strong> be exclusive, e.g. it is undefined behavior for two threads to create a mutable
reference to the same element in a pointer.</li>
</ul>
</li>
<li>Any buffers passed to the kernel must be large enough for the size that the kernel expects. Allocated buffer size being correct
for what the kernel expects is up to the caller, not the kernel.</li>
<li>Not allocating enough dynamic shared memory for how much the kernel expects.</li>
</ul>
<p>Behavior that is not considered undefined but is undesirable:</p>
<ul>
<li>Launching a kernel with more threads than expected by its launch bounds (.maxntid in PTX). This will cause the launch to fail.</li>
<li>Launching a kernel with a different number of threads than expected by its launch bounds (.reqntid in PTX). This will also cause the launch to fail.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-cuda-toolkit"><a class="header" href="#the-cuda-toolkit">The CUDA Toolkit</a></h1>
<p>The CUDA Toolkit is an ecosystem for executing extremely fast code on NVIDIA GPUs for the purpose of general computing.</p>
<p>CUDA includes many libraries for this purpose, including the Driver API, Runtime API, the PTX ISA, libnvvm, etc. CUDA
is currently the best option for computing in terms of libraries and control available, however, it unfortunately only works
on NVIDIA GPUs.</p>
<p>This section will cover some of the general uses of GPU computing, why use CUDA, and general CUDA principles. 
This section will not cover everything about CUDA and it is not meant to. You can check out the <a href="https://docs.nvidia.com/cuda/">official CUDA guide</a>
for a complete overview.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gpu-computing"><a class="header" href="#gpu-computing">GPU Computing</a></h1>
<p>You probably already know what GPU computing is, but if you don't, it is utilizing the extremely parallel
nature of GPUs for purposes other than rendering. It is widely used in many scientific and consumer fields.
Some of the most common uses being fluid/smoke simulation, protein folding, physically based rendering, 
cryptocurrency mining, AI model training, etc. </p>
<p>GPUs excel at tasks that do mostly the same thing every time, and need to do it millions of times. 
They do not excel at so-called &quot;divergent&quot; tasks, tasks where each run of the task may take different amounts
of time and/or take different code paths.</p>
<h1 id="why-cuda"><a class="header" href="#why-cuda">Why CUDA?</a></h1>
<p>CUDA is currently one of the best choices for fast GPU computing for multiple reasons:</p>
<ul>
<li>It offers deep control over how kernels are dispatched and how memory is managed.</li>
<li>It has a rich ecosystem of tutorials, guides, and libraries such as cuRand, cuBlas, libnvvm, optix, the PTX ISA, etc.</li>
<li>It is mostly unmatched in performance because it is solely meant for computing and offers rich control.
And more...</li>
</ul>
<p>However, CUDA can only run on NVIDIA GPUs, which precludes AMD gpus from tools that use it. However, this is a drawback that 
is acceptable by many because of the significant developer cost of supporting both NVIDIA gpus with CUDA and 
AMD gpus with OpenCL, since OpenCL is generally slower, clunkier, and lacks libraries and docs on par with CUDA.</p>
<h1 id="why-rust"><a class="header" href="#why-rust">Why Rust?</a></h1>
<p>Rust is a great choice for GPU programming, however, it has needed a kickstart, which is what rustc_codegen_nvvm tries to 
accomplish; The initial hurdle of getting Rust to compile to something CUDA can run is over, now comes the design and 
polish part. </p>
<p>On top of its rich language features (macros, enums, traits, proc macros, great errors, etc), Rust's safety guarantees
can be applied in gpu programming too; A field that has historically been full of implied invariants and unsafety, such
as (but not limited to):</p>
<ul>
<li>Expecting some amount of dynamic shared memory from the caller.</li>
<li>Expecting a certain layout for thread blocks/threads.</li>
<li>Manually handling the indexing of data, leaving code prone to data races if not managed correctly.</li>
<li>Forgetting to free memory, using uninitialized memory, etc.</li>
</ul>
<p>Not to mention the standardized tooling that makes the building, documentation, sharing, and linting of gpu kernel libraries easily possible.
Most of the reasons for using rust on the CPU apply to using Rust for the GPU, these reasons have been stated countless times so
i will not repeat them here. </p>
<p>A couple of particular rust features make writing CUDA code much easier: RAII and Results.
In <code>cust</code> everything uses RAII (through <code>Drop</code> impls) to manage freeing memory and returning handles, which 
frees users from having to think about that, which yields safer, more reliable code.</p>
<p>Results are particularly helpful, almost every single call in every CUDA library returns a status code in the form of a cuda result.
Ignoring these statuses is very dangerous and can often lead to random segfaults and overall unrealiable code. For this purpose,
both the CUDA SDK, and other libraries provide macros to handle such statuses. This handling is not very reliable and causes
dependency issues down the line. </p>
<p>Instead of an unreliable system of macros, we can leverage rust results for this. In cust we return special <code>CudaResult&lt;T&gt;</code>
results that can be bubbled up using rust's <code>?</code> operator, or, similar to <code>CUDA_SAFE_CALL</code> can be unwrapped or expected if 
proper error handling is not needed. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-cuda-pipeline"><a class="header" href="#the-cuda-pipeline">The CUDA Pipeline</a></h1>
<p>As you may already know, &quot;traditional&quot; cuda is usually in the form of CUDA C/C++ files which use <code>.cu</code> extension. These files 
can be compiled using NVCC (NVIDIA CUDA Compiler) into an executable.</p>
<p>CUDA files consist of <strong>device</strong> and <strong>host</strong> functions. <strong>device</strong> functions are functions that run on the GPU, also called kernels.
<strong>host</strong> functions run on the CPU and usually include logic on how to allocate GPU memory and call device functions.</p>
<p>However, a lot goes on behind the scenes that most people don't know about, a lot of it is integral to how rustc_codegen_nvvm works
so we will briefly go over it.</p>
<h1 id="stages"><a class="header" href="#stages">Stages</a></h1>
<p>The NVIDIA CUDA Compiler consists of distinct stages of compilation:</p>
<p><a href="cuda/graphics/cuda-compilation-from-cu-to-executable.png">![NVCC]</a></p>
<p>NVCC separates device and host functions and compiles them separately. 
Most importantly, device functions are compiled to LLVM IR, and then the LLVM IR is fed to a library
called <code>libnvvm</code>.</p>
<p><code>libnvvm</code> is a closed source library which takes in a subset of LLVM IR, it optimizes it further, then it
turns it into the next and most important stage of compilation, the PTX ISA.</p>
<p>PTX is a low level, assembly-like format with an open specification which can be targeted by any language.</p>
<p>We won't dig deep into what happens after PTX, but in essence, it is turned into a final format called SASS
which is register allocated and is finally sent to the GPU to execute.</p>
<h1 id="libnvvm"><a class="header" href="#libnvvm">libnvvm</a></h1>
<p>The stage/library we are most interested in is <code>libnvvm</code>. libnvvm is a closed source library that is 
distributed in every download of the CUDA SDK. Libnvvm takes a format called NVVM IR, it optimizes it, and 
converts it to a single PTX file you can run on NVIDIA GPUs using the driver or runtime API.</p>
<p>NVVM IR is a subset of LLVM IR, that is to say, it is a version of LLVM IR with restrictions. A couple 
of examples being:</p>
<ul>
<li>Many intrinsics are unsupported</li>
<li>&quot;Irregular&quot; integer types such as <code>i4</code> or <code>i111</code> are unsupported and will segfault (however in theory they should be supported)</li>
<li>Global names cannot include <code>.</code>.</li>
<li>Some linkage types are not supported.</li>
<li>Function ABIs are ignored, everything uses the PTX calling convention.</li>
</ul>
<p>You can find the full specification of the NVVM IR <a href="https://docs.nvidia.com/cuda/nvvm-ir-spec/index.html">here</a> if you are interested.</p>
<h1 id="special-ptx-features"><a class="header" href="#special-ptx-features">Special PTX features</a></h1>
<p>As far as an assembly format goes, PTX is fairly user friendly for a couple of reasons:</p>
<ul>
<li>It is well formatted.</li>
<li>It is mostly fully specified (other than the iffy grammar specification).</li>
<li>It uses named registers/parameters</li>
<li>It uses virtual registers (since gpus have thousands of registers, listing all of them out would be unrealistic).</li>
<li>It uses ASCII as a file encoding.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="technical"><a class="header" href="#technical">Technical</a></h1>
<p>This section will cover the more technical details of how rustc_codegen_nvvm works 
as well as the issues that came with it.</p>
<p>It will also explain some technical details about CUDA/PTX/etc, it is not necessarily 
limited to rustc_codegen_nvvm.</p>
<p>Basic knowledge of how rustc and LLVM work and what they do is assumed. You can find
info about rustc in the <a href="https://rustc-dev-guide.rust-lang.org/">rustc dev guide</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="custom-rustc-backends"><a class="header" href="#custom-rustc-backends">Custom Rustc Backends</a></h1>
<p>Before we get into the details of rustc_codegen_nvvm, we obviously need to explain what a codegen is!</p>
<p>Custom codegens are rustc's answer to &quot;well what if i want rust to compile to X?&quot;. This is a problem
that comes up in many situations, especially conversations of &quot;well LLVM cannot target this, so we are screwed&quot;.
To solve this problem, rustc decided to incrementally decouple itself from being attached/reliant on LLVM exclusively.</p>
<p>Previously, rustc only had a single codegen, the LLVM codegen. The LLVM codegen translated MIR directly to LLVM IR.
This is great if you just want to support LLVM, but LLVM is not perfect, and inevitably you will hit limits to what LLVM
is able to do. Or, you may just want to stop using LLVM, LLVM is not without problems (it is often slow, clunky to deal with, 
and does not support a lot of targets). </p>
<p>Nowadays, Rustc is almost fully decoupled from LLVM and it is instead generic over the &quot;codegen&quot; backend used.
Rustc instead uses a system of codegen backends that implement traits and then get loaded as dynamically linked libraries.
This allows rust to compile to virtually anything with a surprisingly small amount of work. At the time of writing, there are
five publicly known codegens that exist:</p>
<ul>
<li>rustc_codegen_clif, cranelift</li>
<li>rustc_codegen_llvm</li>
<li>rustc_codegen_gcc</li>
<li>rustc_codegen_spirv</li>
<li>rustc_codegen_nvvm, obviously the best codegen ;)</li>
</ul>
<p><code>rustc_codegen_clif</code> targets the cranelift backend, which is a codegen backend written in rust that is faster than LLVM but does not have many optimizations
compared to LLVM. <code>rustc_codegen_llvm</code> is obvious, it is the backend almost everybody uses which targets LLVM. <code>rustc_codegen_gcc</code> targets GCC (GNU Compiler Collection)
which is able to target more exotic targets than LLVM, especially for embedded. <code>rustc_codegen_spirv</code> targets the SPIR-V (Standard Portable Intermediate Representation 5)
format, which is a format mostly used for compiling shader languages such as GLSL or WGSL to a standard representation that Vulkan/OpenGL can use, the reasons
why SPIR-V is not an alternative to CUDA/rustc_codegen_nvvm have been covered in the <a href="nvvm/technical/../../faq.html">FAQ</a>.</p>
<p>Finally, we come to the star of the show, <code>rustc_codegen_nvvm</code>. This backend targets NVVM IR for compiling rust to gpu kernels that can be run by CUDA. 
What NVVM IR/libnvvm are has been covered in the <a href="nvvm/technical/../../cuda/pipeline.html">CUDA section</a>.</p>
<h1 id="rustc_codegen_ssa"><a class="header" href="#rustc_codegen_ssa">rustc_codegen_ssa</a></h1>
<p>Despite its name, <code>rustc_codegen_ssa</code> does not actually codegen to anything, it is however the central crate behind every single codegen.
The SSA codegen does most of the hard work in codegen, which is actually codegenning MIR and taking care of managing codegen altogether.</p>
<p>The SSA codegen abstracts away the MIR lowering logic so that custom codegens do not have to implement the time consuming logic of lowering MIR,
they can just implement a bunch of traits and the SSA codegen does everything else.</p>
<p>The SSA codegen is literally just a bunch of traits, for example:</p>
<ul>
<li>A trait for getting a type like an integer type</li>
<li>A trait for optimizing a module</li>
<li>A trait for linking everything</li>
<li>A trait for declaring a function
...etc
You will find an SSA codegen trait in almost every single file.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rustc_codegen_nvvm"><a class="header" href="#rustc_codegen_nvvm">rustc_codegen_nvvm</a></h1>
<p>At the highest level, our codegen workflow goes like this:</p>
<pre><code>Source code -&gt; Typechecking -&gt; MIR -&gt; SSA Codegen -&gt; LLVM IR (NVVM IR) -&gt; PTX -&gt; PTX opts/function DCE -&gt; Final PTX
               |                                     |                  |      |                                  ^
               |                                     |          libnvvm +------+                                  |
               |                                     |                                                            |
               |                  rustc_codegen_nvvm +------------------------------------------------------------|
         Rustc +---------------------------------------------------------------------------------------------------
</code></pre>
<p>Before we do anything, rustc does its normal job, it typechecks, converts everything to MIR, etc. Then, 
rustc loads our codegen shared lib and invokes it to codegen the MIR. It creates an instance of
<code>NvvmCodegenBackend</code> and it invokes <code>codegen_crate</code>. You could do anything inside <code>codegen_crate</code> but 
we just defer back to rustc_codegen_ssa and tell it to do the job for us:</p>
<pre><code class="language-rs">fn codegen_crate&lt;'tcx&gt;(
    &amp;self,
    tcx: TyCtxt&lt;'tcx&gt;,
    metadata: EncodedMetadata,
    need_metadata_module: bool,
) -&gt; Box&lt;dyn std::any::Any&gt; {
    Box::new(rustc_codegen_ssa::base::codegen_crate(
        NvvmCodegenBackend,
        tcx,
        String::new(),
        metadata,
        need_metadata_module,
    ))
}
</code></pre>
<p>After that, the codegen logic is kind of abstracted away from us, which is a good thing!
We just need to provide the SSA codegen whatever it needs to do its thing. This is 
done in the form of traits, lots and lots and lots of traits, more traits than you've ever seen, traits
your subconscious has warned you of in nightmares, anyways. Because talking about how the SSA codegen
works is kind of useless, we will instead talk first about general concepts and terminology, then 
dive into each trait. </p>
<p>But first, let's talk about the end of the codegen, it is pretty simple, we do a couple of things:
<em>after codegen is done and LLVM has been run to optimize each module</em></p>
<ul>
<li>1: We gather every llvm bitcode module we created.</li>
<li>2: We create a new libnvvm program.</li>
<li>3: We add every bitcode module to the libnvvm program.</li>
<li>4: We try to find libdevice and add it to the program (see <a href="https://docs.nvidia.com/cuda/libdevice-users-guide/introduction.html#what-is-libdevice">nvidia docs</a> on what libdevice is).</li>
<li>5: We run the verifier on the nvvm program just to check that we did not create any invalid nvvm ir.</li>
<li>6: We run the compiler which gives us a final PTX string, hooray!</li>
<li>7: Finally, the PTX goes through a small stage where its parsed and function DCE is run to eliminate
Most of the bloat in the file, traditionally this is done by the linker but theres no linker to be found for miles here.</li>
<li>8: We write this ptx file to wherever rustc tells us to write the final file.</li>
</ul>
<p>We will cover the libnvvm steps in more detail later on.</p>
<h1 id="codegen-units-cgus"><a class="header" href="#codegen-units-cgus">Codegen Units (CGUs)</a></h1>
<p>Ah codegen units, the thing everyone just tells you to set to <code>1</code> in Cargo.toml, but what are they?
Well, to put it simply, codegen units are rustc splitting up a crate into different modules to then 
run LLVM in parallel over. For example, rustc can run LLVM over two different modules in parallel and 
save time.</p>
<p>This gets a little bit more complex with generics, because MIR is not monomorphized and monomorphized MIR is not a thing,
the codegen monomorphizes instances on the fly. Therefore rustc needs to put any generic functions that one CGU relies on
inside of the same CGU because it needs to monomorphize them.</p>
<h1 id="rlibs"><a class="header" href="#rlibs">Rlibs</a></h1>
<p>rlibs are mysterious files, their origins are mysterious and their contents are the deepest layer of the iceberg. Just kidding,
but rlibs often confuse people (including me at first). Rlibs are rustc's way of encoding basically everything it needs to know 
about a crate into a file. Rlibs usually contain the following:</p>
<ul>
<li>Object files for each CGU.</li>
<li>LLVM Bitcode.</li>
<li>a Symbol table.</li>
<li>metadata:
<ul>
<li>rustc version (because things can go kaboom if version mismatches, ABIs are fun amirite)</li>
<li>A crate hash</li>
<li>a crate id</li>
<li>info about the source files</li>
<li>the exported API, things like macros, traits, etc.</li>
<li>MIR, for things such as generic functions and <code>#[inline]</code>d functions (please don't put <code>#[inline]</code> on everything, rustc will cry)</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="types"><a class="header" href="#types">Types</a></h1>
<p>Types! who doesn't love types, especially those that cause libnvvm to randomly segfault or loop forever!
Anyways, types are an integral part of the codegen and everything revolves around them and you will see them everywhere.</p>
<p><code>rustc_codegen_ssa</code> does not actually tell you what your type representation should be, it allows you to decide. For
example, <code>rust-gpu</code> represents it as a <code>SpirvType</code> enum, while both <code>cg_llvm</code> and our codegen represent it as 
opaque llvm types:</p>
<pre><code class="language-rs">type Type = &amp;'ll llvm::Type;
</code></pre>
<p><code>llvm::Type</code> is an opaque type that comes from llvm-c. <code>'ll</code> is one of the main lifetimes you will see
throughout the whole codegen, it is used for anything that lasts as long as the current usage of llvm. 
LLVM gives you back pointers when you ask for a type or value, some time ago <code>cg_llvm</code> fully switched to using
references over pointers, and we follow in their footsteps. </p>
<p>One important fact about types is that they are opaque, you cannot take a type and ask &quot;is this X struct?&quot;,
this is like asking &quot;which chickens were responsible for this omelette?&quot;. You can ask if its a number type,
a vector type, a void type, etc. </p>
<p>The SSA codegen needs to ask the backend for types for everything it needs to codegen MIR. It does 
this using a trait called <code>BaseTypeMethods</code>:</p>
<pre><code class="language-rs">pub trait BaseTypeMethods&lt;'tcx&gt;: Backend&lt;'tcx&gt; {
    fn type_i1(&amp;self) -&gt; Self::Type;
    fn type_i8(&amp;self) -&gt; Self::Type;
    fn type_i16(&amp;self) -&gt; Self::Type;
    fn type_i32(&amp;self) -&gt; Self::Type;
    fn type_i64(&amp;self) -&gt; Self::Type;
    fn type_i128(&amp;self) -&gt; Self::Type;
    fn type_isize(&amp;self) -&gt; Self::Type;

    fn type_f32(&amp;self) -&gt; Self::Type;
    fn type_f64(&amp;self) -&gt; Self::Type;

    fn type_func(&amp;self, args: &amp;[Self::Type], ret: Self::Type) -&gt; Self::Type;
    fn type_struct(&amp;self, els: &amp;[Self::Type], packed: bool) -&gt; Self::Type;
    fn type_kind(&amp;self, ty: Self::Type) -&gt; TypeKind;
    fn type_ptr_to(&amp;self, ty: Self::Type) -&gt; Self::Type;
    fn type_ptr_to_ext(&amp;self, ty: Self::Type, address_space: AddressSpace) -&gt; Self::Type;
    fn element_type(&amp;self, ty: Self::Type) -&gt; Self::Type;

    /// Returns the number of elements in `self` if it is a LLVM vector type.
    fn vector_length(&amp;self, ty: Self::Type) -&gt; usize;

    fn float_width(&amp;self, ty: Self::Type) -&gt; usize;

    /// Retrieves the bit width of the integer type `self`.
    fn int_width(&amp;self, ty: Self::Type) -&gt; u64;

    fn val_ty(&amp;self, v: Self::Value) -&gt; Self::Type;
}
</code></pre>
<p>Every codegen implements this some way or another, you can find our implementation in <code>ty.rs</code>. Our
implementation is pretty straightforward, LLVM has functions that we link to which get us the types we need:</p>
<pre><code class="language-rs">impl&lt;'ll, 'tcx&gt; BaseTypeMethods&lt;'tcx&gt; for CodegenCx&lt;'ll, 'tcx&gt; {
    fn type_i1(&amp;self) -&gt; &amp;'ll Type {
        unsafe { llvm::LLVMInt1TypeInContext(self.llcx) }
    }

    fn type_i8(&amp;self) -&gt; &amp;'ll Type {
        unsafe { llvm::LLVMInt8TypeInContext(self.llcx) }
    }

    fn type_i16(&amp;self) -&gt; &amp;'ll Type {
        unsafe { llvm::LLVMInt16TypeInContext(self.llcx) }
    }

    fn type_i32(&amp;self) -&gt; &amp;'ll Type {
        unsafe { llvm::LLVMInt32TypeInContext(self.llcx) }
    }

    fn type_i64(&amp;self) -&gt; &amp;'ll Type {
        unsafe { llvm::LLVMInt64TypeInContext(self.llcx) }
    }

    fn type_i128(&amp;self) -&gt; &amp;'ll Type {
        unsafe { llvm::LLVMIntTypeInContext(self.llcx, 128) }
    }

    fn type_isize(&amp;self) -&gt; &amp;'ll Type {
        self.isize_ty
    }

    fn type_f32(&amp;self) -&gt; &amp;'ll Type {
        unsafe { llvm::LLVMFloatTypeInContext(self.llcx) }
    }

    fn type_f64(&amp;self) -&gt; &amp;'ll Type {
        unsafe { llvm::LLVMDoubleTypeInContext(self.llcx) }
    }

    fn type_func(&amp;self, args: &amp;[&amp;'ll Type], ret: &amp;'ll Type) -&gt; &amp;'ll Type {
        unsafe { llvm::LLVMFunctionType(ret, args.as_ptr(), args.len() as c_uint, False) }
    }

// ...
</code></pre>
<p>There is also logic for handling ABI types, such as generating aggregate (struct) types </p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
    </body>
</html>
